{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考: https://qiita.com/sugulu_Ogawa_ISID/items/697bd03499c1de9cf082"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0+cu113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import optuna\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "print(torch.__version__)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of              id  topic              status            label  \\\n",
       "260782  1152146  10026  649586508110757888  [0, 0, 0, 0, 1]   \n",
       "328485  1394459  10022  703752483483430912  [0, 0, 1, 0, 0]   \n",
       "493161  2672788  10021  691170800846548992  [0, 0, 0, 1, 0]   \n",
       "83628    478577  10000  668649960359878656  [0, 0, 1, 0, 0]   \n",
       "57522    473962  10000  667307740990083073  [0, 0, 0, 1, 0]   \n",
       "...         ...    ...                 ...              ...   \n",
       "40864    457206  10000  661404228057849856  [0, 0, 0, 0, 1]   \n",
       "258538  1154205  10024  460976864593326081  [0, 0, 0, 1, 0]   \n",
       "15994     28434  10000  527276031233646592  [0, 0, 1, 1, 0]   \n",
       "469170  2152918  10025  699154791004409856  [0, 0, 0, 1, 0]   \n",
       "413335  2021103  10024  678222887090700288  [0, 0, 0, 1, 0]   \n",
       "\n",
       "                                                     text  \n",
       "260782                   コーテク垢までシャープに絡みに行ってるやんけｗｗｗｗｗｗｗｗｗｗ  \n",
       "328485  パナ機今日も録画準備に入った所で電源落ちたよね\\nJスポ4CC男子フリーの頭切れてるよね\\n...  \n",
       "493161            寒波により強気なので\\nとりあえず、OLSでiPhone6sを\\n申込みました  \n",
       "83628   リリイベで当選メールが必要な今日になって再起動病再発するxperia z5ちゃんにキレそう(...  \n",
       "57522                                                      \n",
       "...                                                   ...  \n",
       "40864                                       ちなみにおれはXperia  \n",
       "258538                                                     \n",
       "15994                                                      \n",
       "469170                                                     \n",
       "413335  コンビニのネットプリント使えば、対象のコンビニにさえ行ければ、誰でも会報が入手できる…。こう...  \n",
       "\n",
       "[7000 rows x 5 columns]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(pd.read_pickle('./data/twitter/twitterJSA_data.pickle'))\n",
    "lable2idx = {0:\"ポジティブ&ネガティブ\",\n",
    "            1:\"ポジティブ\",\n",
    "            2:\"ネガティブ\",\n",
    "            3:\"ニュートラル\",\n",
    "            4:\"無関係\"}\n",
    "label_length = 5\n",
    "\n",
    "# data数削減\n",
    "df = df.sample(n=7000, random_state=123)\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 1]    2019\n",
      "[0, 0, 0, 1, 0]    1451\n",
      "[0, 0, 1, 0, 0]     170\n",
      "[0, 1, 0, 0, 0]      98\n",
      "[0, 0, 0, 1, 1]      95\n",
      "[0, 0, 1, 1, 0]      32\n",
      "[0, 1, 0, 1, 0]      19\n",
      "[0, 0, 1, 0, 1]      13\n",
      "[0, 1, 0, 0, 1]      11\n",
      "[1, 0, 0, 0, 0]       5\n",
      "[1, 0, 0, 1, 0]       3\n",
      "[1, 1, 0, 0, 0]       2\n",
      "[1, 1, 1, 1, 1]       2\n",
      "Name: label, dtype: int64\n",
      "[0, 0, 0, 0, 1]    1095\n",
      "[0, 0, 0, 1, 0]     765\n",
      "[0, 0, 1, 0, 0]      93\n",
      "[0, 1, 0, 0, 0]      49\n",
      "[0, 0, 0, 1, 1]      49\n",
      "[0, 0, 1, 1, 0]      16\n",
      "[0, 0, 1, 0, 1]      10\n",
      "[0, 1, 0, 1, 0]       8\n",
      "[1, 0, 0, 0, 0]       5\n",
      "[0, 1, 0, 0, 1]       5\n",
      "[1, 0, 0, 1, 0]       2\n",
      "[1, 1, 0, 0, 0]       2\n",
      "[1, 0, 0, 0, 1]       1\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.3, shuffle=True)\n",
    "train_df, valid_df = train_test_split(train_df, test_size=0.2, shuffle=True)\n",
    "print(train_df[\"label\"].value_counts())\n",
    "print(test_df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForMaskedLM, AutoModelForSequenceClassification\n",
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification, BertJapaneseTokenizer, BertForSequenceClassification, ElectraForPreTraining\n",
    "# model_path = \"cl-tohoku/bert-base-japanese\"\n",
    "model_path = \"izumi-lab/electra-base-japanese-discriminator\"\n",
    "# model_path = \"izumi-lab/electra-small-japanese-fin-discriminator\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at izumi-lab/electra-base-japanese-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at izumi-lab/electra-base-japanese-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nlast_layer = list(electra_model.children())[-1]\\nprint(f'except last layer: {last_layer}')\\nfor param in last_layer.parameters():\\n    param.requires_grad = True\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertJapaneseTokenizer.from_pretrained(model_path)\n",
    "electra_model = ElectraForPreTraining.from_pretrained(model_path)\n",
    "classifar_model = AutoModelForSequenceClassification.from_pretrained(model_path, problem_type=\"multi_label_classification\", num_labels=label_length)\n",
    "\n",
    "# freeze layers except last layer\n",
    "for param in electra_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "'''\n",
    "last_layer = list(electra_model.children())[-1]\n",
    "print(f'except last layer: {last_layer}')\n",
    "for param in last_layer.parameters():\n",
    "    param.requires_grad = True\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "train_encoding = tokenizer(train_df[\"text\"].to_list(), return_tensors=\"pt\",padding=True, truncation=True, max_length=max_len)\n",
    "valid_encoding = tokenizer(valid_df[\"text\"].to_list(), return_tensors=\"pt\",padding=True, truncation=True, max_length=max_len)\n",
    "test_encoding = tokenizer(test_df[\"text\"].to_list(), return_tensors=\"pt\",padding=True, truncation=True, max_length=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = torch.tensor(train_df[\"label\"].to_list())\n",
    "valid_label = torch.tensor(valid_df[\"label\"].to_list())\n",
    "test_label = torch.tensor(test_df[\"label\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    1, 20109,    70, 20670, 13356, 12082,   700, 13951,   720,   767,\n",
      "            2,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3])\n",
      "[CLS] そもそも Xperia シリーズ が 好き だ わ [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "# 0 番目 の 5 トーク ン までの ID を 表示\n",
    "print(train_encoding[\"input_ids\"][ 0][: 50]) \n",
    "# 0 番目 の 5 トーク ン までの トー クン を デコード し て 表示\n",
    "print(tokenizer.decode(train_encoding[\"input_ids\"][0][:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__( self, idx):\n",
    "        item = {key:torch.tensor(val[idx]).clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float32).clone().detach()\n",
    "        return item\n",
    "    \n",
    "    def __len__( self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = CreateDataset(train_encoding, train_label)\n",
    "val_dataset = CreateDataset(valid_encoding, valid_label)\n",
    "test_dataset = CreateDataset(test_encoding, test_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = 64, shuffle = True)\n",
    "validate_loader = DataLoader(val_dataset, batch_size = 64, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(model): \n",
    "    path = \"./IzumiElectraModel_multilabel_bceloss.pth\" \n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(trial, model):\n",
    "  optimizer_names = ['Adam', 'MomentumSGD'] # 'rmsprop'\n",
    "  optimizer_name = trial.suggest_categorical('optimizer', optimizer_names)\n",
    "  \n",
    "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-4)\n",
    "  \n",
    "  if optimizer_name == optimizer_names[0]: \n",
    "    adam_lr = trial.suggest_loguniform('adam_lr', 1e-4, 5e-3)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=adam_lr, weight_decay=weight_decay)\n",
    "  elif optimizer_name == optimizer_names[1]:\n",
    "    momentum_sgd_lr = trial.suggest_loguniform('momentum_sgd_lr', 1e-4, 5e-3)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=momentum_sgd_lr, momentum=0.9, weight_decay=weight_decay)\n",
    "  else:\n",
    "    optimizer = optim.RMSprop(model.parameters())\n",
    "  \n",
    "  return optimizer\n",
    "\n",
    "\n",
    "def get_activation(trial):\n",
    "    activation_names = ['ReLU', 'ELU', 'GELU']\n",
    "    activation_name = trial.suggest_categorical('activation', activation_names)\n",
    "    \n",
    "    if activation_name == activation_names[0]:\n",
    "      activation = F.relu\n",
    "    elif activation_name == activation_names[1]:\n",
    "      activation = F.elu\n",
    "    else:\n",
    "      activation = F.gelu\n",
    "    return activation\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, trial):\n",
    "      super().__init__()\n",
    "      self.electra_model = electra_model\n",
    "      self.activation_func = get_activation(trial) # relu elu gelu\n",
    "      # 第1層\n",
    "      self.fc1 = nn.Linear(768, 500)\n",
    "      self.dropout1 = nn.Dropout(0.1)\n",
    "      # 第2層\n",
    "      self.fc2 = nn.Linear(500, 500)\n",
    "      self.dropout2 = nn.Dropout(0.1)\n",
    "      # 第3層\n",
    "      self.fc3 = nn.Linear(500, 300)\n",
    "      self.dropout3 = nn.Dropout(0.1)\n",
    "      # 第4層\n",
    "      self.fc4 = nn.Linear(300, label_length)\n",
    "      \n",
    "    def forward(self, x):\n",
    "      x = self.activation_func(self.fc1(x))\n",
    "      x = self.dropout1(x)\n",
    "      x = self.activation_func(self.fc2(x))\n",
    "      x = self.dropout2(x)\n",
    "      x = self.activation_func(self.fc3(x))\n",
    "      x = self.dropout3(x)\n",
    "      output = self.activation_func(self.fc4(x))\n",
    "      return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_func = torch.nn.Sigmoid()\n",
    "loss_func = torch.nn.BCELoss()\n",
    "EVAL_FUNC = True # if using \"Exact Match\" then \"True\", else using \"Parts Match\" then \"False\"\n",
    "\n",
    "def train(model, device, train_loader, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = electra_model(input_ids, attention_mask = attention_mask, output_hidden_states=True)\n",
    "        last_hidden_states = outputs.hidden_states[-1][:,0,:]\n",
    "        logits = model(last_hidden_states)\n",
    "        loss = loss_func(sigmoid_func(logits), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)\n",
    "        \n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        acc_list = []\n",
    "        losses_list = []\n",
    "        running_loss = 0\n",
    "        total = 0\n",
    "        probs_list = []\n",
    "        labels_list = []\n",
    "\n",
    "        for batch in tqdm(test_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels']\n",
    "            outputs = electra_model(input_ids, attention_mask = attention_mask, output_hidden_states=True)\n",
    "            last_hidden_states = outputs.hidden_states[-1][:,0,:]\n",
    "            logits = model(last_hidden_states)\n",
    "            test_loss = loss_func(sigmoid_func(logits).to('cpu'), labels)\n",
    "            \n",
    "            probs = sigmoid_func(logits).to('cpu')\n",
    "            probs_list.append(probs)\n",
    "            labels_list.append(labels.to('cpu'))\n",
    "\n",
    "            # The label with the highest value will be our prediction \n",
    "            #_, predicted = torch.max(logits, 1) \n",
    "\n",
    "            total += logits.size(0) * logits.size(1)\n",
    "            running_loss += test_loss.item()\n",
    " \n",
    "        probs_ = torch.cat(probs_list, 0)\n",
    "        labels_ = torch.cat(labels_list, 0)\n",
    "        for th in range(1, 10, 1):\n",
    "            correct = 0\n",
    "            predicted = torch.where(probs_ > th/10, torch.ones(len(labels_), label_length), torch.zeros(len(labels_), label_length))\n",
    "            if EVAL_FUNC:\n",
    "                correct += (predicted == predicted).all(axis=1).sum().item()\n",
    "                acc_list.append(correct / len(labels_))\n",
    "            else:\n",
    "                correct += (predicted == labels_).sum().item()\n",
    "                acc_list.append(correct / total)\n",
    "\n",
    "    max_acc = max(acc_list)\n",
    "    max_acc_index = acc_list.index(max_acc)\n",
    "\n",
    "\n",
    "    print(f\"max_acc : {max_acc} then th=0.{max_acc_index+1}\")\n",
    "    return running_loss/len(test_loader), acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 20\n",
    "def objective(trial):\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "  mlp_model= MLP(trial).to(device)\n",
    "  optimizer = get_optimizer(trial, mlp_model)\n",
    "\n",
    "  epoc_profile = {}\n",
    "  max_acc = 0\n",
    "  for step in range(EPOCH):\n",
    "    train_loss = train(mlp_model, device, train_loader, optimizer)\n",
    "    test_loss, acc_list = test(mlp_model, device, test_loader)\n",
    "\n",
    "    epoc_profile[f\"epoc{step}_test_loss\"] = test_loss\n",
    "    epoc_profile[f\"epoc{step}_acc\"] = acc_list\n",
    "    epoc_profile[f\"epoc{step}_train_loss\"] = train_loss\n",
    "\n",
    "    trial.set_user_attr(f'profile', epoc_profile)\n",
    "    if max(acc_list) > max_acc:\n",
    "      torch.save(mlp_model.state_dict(), f\"./models/checkpoints/trialnum_{trial.number+1}.pth\")\n",
    "      max_acc = max(acc_list)\n",
    "  return (1 - max(acc_list))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-08-25 12:51:47,338]\u001b[0m A new study created in memory with name: no-name-b7449556-f368-46be-a6bc-044423523db7\u001b[0m\n",
      "  0%|          | 0/62 [00:00<?, ?it/s]C:\\Users\\windo\\AppData\\Local\\Temp\\ipykernel_32360\\825136160.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key:torch.tensor(val[idx]).clone().detach() for key, val in self.encodings.items()}\n",
      "C:\\Users\\windo\\AppData\\Local\\Temp\\ipykernel_32360\\825136160.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float32).clone().detach()\n",
      "100%|██████████| 62/62 [00:09<00:00,  6.77it/s]\n",
      "100%|██████████| 33/33 [00:04<00:00,  6.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_acc : 1.0 then th=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 34/62 [00:06<00:05,  5.15it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mセル16 を e:\\workspace\\news_classification\\run_electra_multilabel_optuna.ipynb\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/workspace/news_classification/run_electra_multilabel_optuna.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m TRIAL_SIZE \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/workspace/news_classification/run_electra_multilabel_optuna.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/workspace/news_classification/run_electra_multilabel_optuna.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49mTRIAL_SIZE)\n",
      "File \u001b[1;32me:\\anaconda_env\\nlp\\lib\\site-packages\\optuna\\study\\study.py:400\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[39mif\u001b[39;00m n_jobs \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    393\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    394\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`n_jobs` argument has been deprecated in v2.7.0. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    395\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis feature will be removed in v4.0.0. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    396\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/optuna/optuna/releases/tag/v2.7.0.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    397\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    398\u001b[0m     )\n\u001b[1;32m--> 400\u001b[0m _optimize(\n\u001b[0;32m    401\u001b[0m     study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    402\u001b[0m     func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    403\u001b[0m     n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    404\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    405\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    406\u001b[0m     catch\u001b[39m=\u001b[39;49mcatch,\n\u001b[0;32m    407\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    408\u001b[0m     gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    409\u001b[0m     show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    410\u001b[0m )\n",
      "File \u001b[1;32me:\\anaconda_env\\nlp\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m show_progress_bar:\n",
      "File \u001b[1;32me:\\anaconda_env\\nlp\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[1;32me:\\anaconda_env\\nlp\\lib\\site-packages\\optuna\\study\\_optimize.py:213\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    210\u001b[0m     thread\u001b[39m.\u001b[39mstart()\n\u001b[0;32m    212\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 213\u001b[0m     value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    214\u001b[0m \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "\u001b[1;32mセル16 を e:\\workspace\\news_classification\\run_electra_multilabel_optuna.ipynb\u001b[0m in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/workspace/news_classification/run_electra_multilabel_optuna.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m max_acc \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/workspace/news_classification/run_electra_multilabel_optuna.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCH):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/workspace/news_classification/run_electra_multilabel_optuna.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m   train_loss \u001b[39m=\u001b[39m train(mlp_model, device, train_loader, optimizer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/workspace/news_classification/run_electra_multilabel_optuna.ipynb#X21sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m   test_loss, acc_list \u001b[39m=\u001b[39m test(mlp_model, device, test_loader)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/workspace/news_classification/run_electra_multilabel_optuna.ipynb#X21sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m   epoc_profile[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoc\u001b[39m\u001b[39m{\u001b[39;00mstep\u001b[39m}\u001b[39;00m\u001b[39m_test_loss\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m test_loss\n",
      "\u001b[1;32mセル16 を e:\\workspace\\news_classification\\run_electra_multilabel_optuna.ipynb\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, device, train_loader, optimizer)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/workspace/news_classification/run_electra_multilabel_optuna.ipynb#X21sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/workspace/news_classification/run_electra_multilabel_optuna.ipynb#X21sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/workspace/news_classification/run_electra_multilabel_optuna.ipynb#X21sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/workspace/news_classification/run_electra_multilabel_optuna.ipynb#X21sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mreturn\u001b[39;00m running_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_loader)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TRIAL_SIZE = 2\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=TRIAL_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'ELU',\n",
       " 'optimizer': 'Adam',\n",
       " 'weight_decay': 1.0114777524504494e-05,\n",
       " 'adam_lr': 0.0012947367272763107}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenTrial(number=0, values=[0.47619047619047616], datetime_start=datetime.datetime(2022, 8, 25, 11, 58, 35, 34141), datetime_complete=datetime.datetime(2022, 8, 25, 12, 5, 15, 644978), params={'activation': 'ELU', 'optimizer': 'Adam', 'weight_decay': 1.0114777524504494e-05, 'adam_lr': 0.0012947367272763107}, distributions={'activation': CategoricalDistribution(choices=('ReLU', 'ELU', 'GELU')), 'optimizer': CategoricalDistribution(choices=('Adam', 'MomentumSGD')), 'weight_decay': LogUniformDistribution(high=0.0001, low=1e-06), 'adam_lr': LogUniformDistribution(high=0.005, low=0.0001)}, user_attrs={'profile': {'epoc0_test_loss': 0.49892083442572394, 'epoc0_acc': [0.0, 0.0, 0.023333333333333334, 0.36428571428571427, 0.36428571428571427, 0.0, 0.0, 0.0, 0.0], 'epoc0_train_loss': 0.5171533077955246, 'epoc1_test_loss': 0.47721914450327557, 'epoc1_acc': [0.0, 0.0, 0.023333333333333334, 0.45714285714285713, 0.42428571428571427, 0.0, 0.0, 0.0, 0.0], 'epoc1_train_loss': 0.48781480327729254, 'epoc2_test_loss': 0.4740036138982484, 'epoc2_acc': [0.0, 0.0, 0.056666666666666664, 0.4657142857142857, 0.5095238095238095, 0.4038095238095238, 0.0680952380952381, 0.009523809523809525, 0.0], 'epoc2_train_loss': 0.4775905003470759, 'epoc3_test_loss': 0.47442062424890924, 'epoc3_acc': [0.0, 0.0, 0.056666666666666664, 0.4542857142857143, 0.5214285714285715, 0.40285714285714286, 0.06714285714285714, 0.03571428571428571, 0.012380952380952381], 'epoc3_train_loss': 0.47728783420978055, 'epoc4_test_loss': 0.47197949344461615, 'epoc4_acc': [0.0, 0.0, 0.05904761904761905, 0.5042857142857143, 0.3238095238095238, 0.04476190476190476, 0.03333333333333333, 0.02, 0.007142857142857143], 'epoc4_train_loss': 0.4758449302565667, 'epoc5_test_loss': 0.46895648313291144, 'epoc5_acc': [0.0, 0.0, 0.09142857142857143, 0.4666666666666667, 0.5266666666666666, 0.4166666666666667, 0.08476190476190476, 0.051904761904761905, 0.02857142857142857], 'epoc5_train_loss': 0.4732176219263384, 'epoc6_test_loss': 0.47333553072178003, 'epoc6_acc': [0.0, 0.0, 0.04666666666666667, 0.47714285714285715, 0.4895238095238095, 0.13666666666666666, 0.02619047619047619, 0.007619047619047619, 0.0009523809523809524], 'epoc6_train_loss': 0.472767882289425, 'epoc7_test_loss': 0.46902352481177356, 'epoc7_acc': [0.0, 0.0, 0.09952380952380953, 0.46904761904761905, 0.4861904761904762, 0.4076190476190476, 0.09, 0.06476190476190476, 0.04666666666666667], 'epoc7_train_loss': 0.4717318531005613, 'epoc8_test_loss': 0.4698567300131827, 'epoc8_acc': [0.0, 0.0, 0.10428571428571429, 0.5247619047619048, 0.24428571428571427, 0.18571428571428572, 0.1219047619047619, 0.054285714285714284, 0.03380952380952381], 'epoc8_train_loss': 0.4699603722941491, 'epoc9_test_loss': 0.4864867537310629, 'epoc9_acc': [0.0, 0.0, 0.13095238095238096, 0.24714285714285714, 0.23809523809523808, 0.2123809523809524, 0.14, 0.04, 0.011428571428571429], 'epoc9_train_loss': 0.46886576279517145, 'epoc10_test_loss': 0.4668681991822792, 'epoc10_acc': [0.0, 0.0, 0.10666666666666667, 0.520952380952381, 0.5042857142857143, 0.08666666666666667, 0.05523809523809524, 0.0419047619047619, 0.030952380952380953], 'epoc10_train_loss': 0.47162656245693085, 'epoc11_test_loss': 0.48604925473531085, 'epoc11_acc': [0.0, 0.0, 0.23523809523809525, 0.5433333333333333, 0.48857142857142855, 0.20285714285714285, 0.17476190476190476, 0.14714285714285713, 0.11476190476190476], 'epoc11_train_loss': 0.47041028545748803, 'epoc12_test_loss': 0.4698943423502373, 'epoc12_acc': [0.0, 0.0, 0.07380952380952381, 0.4961904761904762, 0.49666666666666665, 0.051904761904761905, 0.041428571428571426, 0.037142857142857144, 0.020476190476190478], 'epoc12_train_loss': 0.4719258128635345, 'epoc13_test_loss': 0.4631041351592902, 'epoc13_acc': [0.0, 0.0, 0.12333333333333334, 0.5190476190476191, 0.5304761904761904, 0.4266666666666667, 0.10238095238095238, 0.07380952380952381, 0.05476190476190476], 'epoc13_train_loss': 0.4689784646034241, 'epoc14_test_loss': 0.46059864940065326, 'epoc14_acc': [0.0, 0.0, 0.09761904761904762, 0.4685714285714286, 0.57, 0.4357142857142857, 0.07857142857142857, 0.05476190476190476, 0.04238095238095238], 'epoc14_train_loss': 0.4685787332634772, 'epoc15_test_loss': 0.46836834784710046, 'epoc15_acc': [0.0, 0.0, 0.09666666666666666, 0.5228571428571429, 0.5152380952380953, 0.09571428571428571, 0.06190476190476191, 0.05047619047619047, 0.037142857142857144], 'epoc15_train_loss': 0.46755903718932984, 'epoc16_test_loss': 0.46348448052550806, 'epoc16_acc': [0.0, 0.0, 0.11285714285714285, 0.5123809523809524, 0.5204761904761904, 0.11523809523809524, 0.04857142857142857, 0.03571428571428571, 0.02], 'epoc16_train_loss': 0.4680972200247549, 'epoc17_test_loss': 0.4622891762039878, 'epoc17_acc': [0.0, 0.0, 0.09619047619047619, 0.49333333333333335, 0.5666666666666667, 0.4661904761904762, 0.08476190476190476, 0.048095238095238094, 0.034761904761904765], 'epoc17_train_loss': 0.4664876461029053, 'epoc18_test_loss': 0.4655352370305495, 'epoc18_acc': [0.0, 0.0, 0.15, 0.2776190476190476, 0.5366666666666666, 0.2019047619047619, 0.12761904761904763, 0.06619047619047619, 0.037142857142857144], 'epoc18_train_loss': 0.4682926059730591, 'epoc19_test_loss': 0.4729719207142339, 'epoc19_acc': [0.0, 0.0, 0.054285714285714284, 0.18476190476190477, 0.5238095238095238, 0.047142857142857146, 0.005238095238095238, 0.0, 0.0], 'epoc19_train_loss': 0.4698581825340948}}, system_attrs={}, intermediate_values={}, trial_id=0, state=TrialState.COMPLETE, value=None)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df = study.trials_dataframe(multi_index=True)\n",
    "hist_df.to_pickle(\"log/profile.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th colspan=\"5\" halign=\"left\">params</th>\n",
       "      <th>user_attrs</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>adam_lr</th>\n",
       "      <th>momentum_sgd_lr</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>profile</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>2022-08-25 11:58:35.034141</td>\n",
       "      <td>2022-08-25 12:05:15.644978</td>\n",
       "      <td>0 days 00:06:40.610837</td>\n",
       "      <td>ELU</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>{'epoc0_test_loss': 0.49892083442572394, 'epoc...</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.478571</td>\n",
       "      <td>2022-08-25 12:05:15.645975</td>\n",
       "      <td>2022-08-25 12:11:59.842367</td>\n",
       "      <td>0 days 00:06:44.196392</td>\n",
       "      <td>GELU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00076</td>\n",
       "      <td>MomentumSGD</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>{'epoc0_test_loss': 0.686696758776, 'epoc0_acc...</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  number     value             datetime_start          datetime_complete  \\\n",
       "                                                                           \n",
       "0      0  0.476190 2022-08-25 11:58:35.034141 2022-08-25 12:05:15.644978   \n",
       "1      1  0.478571 2022-08-25 12:05:15.645975 2022-08-25 12:11:59.842367   \n",
       "\n",
       "                duration     params                                         \\\n",
       "                         activation   adam_lr momentum_sgd_lr    optimizer   \n",
       "0 0 days 00:06:40.610837        ELU  0.001295             NaN         Adam   \n",
       "1 0 days 00:06:44.196392       GELU       NaN         0.00076  MomentumSGD   \n",
       "\n",
       "                                                       user_attrs     state  \n",
       "  weight_decay                                            profile            \n",
       "0     0.000010  {'epoc0_test_loss': 0.49892083442572394, 'epoc...  COMPLETE  \n",
       "1     0.000067  {'epoc0_test_loss': 0.686696758776, 'epoc0_acc...  COMPLETE  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.686696758776"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pro = hist_df[('user_attrs','profile')].to_list()[1]\n",
    "pro['epoc0_test_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoc0_test_loss', 'epoc0_acc', 'epoc0_train_loss', 'epoc1_test_loss', 'epoc1_acc', 'epoc1_train_loss', 'epoc2_test_loss', 'epoc2_acc', 'epoc2_train_loss', 'epoc3_test_loss', 'epoc3_acc', 'epoc3_train_loss', 'epoc4_test_loss', 'epoc4_acc', 'epoc4_train_loss', 'epoc5_test_loss', 'epoc5_acc', 'epoc5_train_loss', 'epoc6_test_loss', 'epoc6_acc', 'epoc6_train_loss', 'epoc7_test_loss', 'epoc7_acc', 'epoc7_train_loss', 'epoc8_test_loss', 'epoc8_acc', 'epoc8_train_loss', 'epoc9_test_loss', 'epoc9_acc', 'epoc9_train_loss', 'epoc10_test_loss', 'epoc10_acc', 'epoc10_train_loss', 'epoc11_test_loss', 'epoc11_acc', 'epoc11_train_loss', 'epoc12_test_loss', 'epoc12_acc', 'epoc12_train_loss', 'epoc13_test_loss', 'epoc13_acc', 'epoc13_train_loss', 'epoc14_test_loss', 'epoc14_acc', 'epoc14_train_loss', 'epoc15_test_loss', 'epoc15_acc', 'epoc15_train_loss', 'epoc16_test_loss', 'epoc16_acc', 'epoc16_train_loss', 'epoc17_test_loss', 'epoc17_acc', 'epoc17_train_loss', 'epoc18_test_loss', 'epoc18_acc', 'epoc18_train_loss', 'epoc19_test_loss', 'epoc19_acc', 'epoc19_train_loss'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pro.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pro['epoc8_train_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5955dc64dc45c9f6ad89241066402e9b90de6090510d28e2354caffb11d22191"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
